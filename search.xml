<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F27%2Fpython%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[#python应用篇 现在公众号的案列分析主要步棸 数据爬取 数据清洗 数据探索 数据可视化 (pyecharts)柱状图、直方图、饼图、线状图、热力图、地理图 评论的文本分析 绘制词云图 一、python数据分析语法总结–来自微信公众号文章import pymysql12345db = pymysql.connect(host=&apos;127.0.0.1&apos;, user=&apos;root&apos;, password=&apos;774110919&apos;, port=3306, db=&apos;maoyan&apos;)cursor = db.cursor()sql = &apos;CREATE TABLE IF NOT EXISTS films (name VARCHAR(255) NOT NULL, type VARCHAR(255) NOT NULL, country VARCHAR(255) NOT NULL, length VARCHAR(255) NOT NULL, released VARCHAR(255) NOT NULL, score VARCHAR(255) NOT NULL, people INT NOT NULL, box_office BIGINT NOT NULL, PRIMARY KEY (name))&apos;cursor.execute(sql)db.close() 创建sql表 conn = pymysql.connect(host=’localhost’, user=’root’, password=’774110919’, port=3306, db=’maoyan’, charset=’utf8mb4’)cursor = conn.cursor()sql = “select * from films”db = pd.read_sql(sql, conn)从数据库选出数据表 df=db.sort_values(by=”score”,ascending=False)根据score标签来递减排序 attr = np.array(dom[‘name’][0:10])attr = [“{}”.format(i.replace(‘：致命守护者’, ‘’)) for i in attr]replace(old,new,max): 使用new字符串替换old字符串，max是替换的次数 month_message=groupby([‘month’]) 根据month标签来分组month_com=month_message[‘month’].agg([‘count’])统计分组中同类项的数量并生成一个标签列month_com.reset_index(inplace=True)通过前面的筛选之后行列序号不会再按照序号排列下来需要使用reser_index函数重新列出序号 month_com_last = month_com.sort_index()按照序号从小到大排列下来 v1 = np.array(month_com_last[‘count’])v1 = [“{}”.format(i) for i in v1]先将series变成数组，然后变成列表 db = pd.read_sql(sql, conn)df = db.sort_values(by=”released”, ascending=False)dom = df[[‘name’, ‘released’]]选取DataFrame中的标签为name和released的两列 db[‘sort_num_money’] = db[‘box_office’].rank(ascending=0, method=’dense’)rank来进行排序 apply函数主要是将一个函数应用的一个DataFrame的数据中,当然这里还要注意行和列的区别需要指定(axis=0每行作为数据内容项)可以省略，(axis=1,每列作为数据数据内容项)如：1234567f=lambda x: x.max()- x.min()frame.apply(f)b 0.505220d 1.284980e 1.880011dtype: float64 df=pd.read_csv(‘air_tianjin_2017.csv’,header=None,name=[‘Date’,’m’,’b’,’a’])dom=df[[‘b’,’a’]] 将series转变为array再转变为listv1=np.array(month_com_list[‘mean’])v1=[“{}”.format(int(i)) for i in v1] ##python的实际案例分析python数据分析主要应用领域 信贷风控 营销分析 金融量化大数据风险控制从信贷风控的角度研究数据分析如何帮助公司创收？信贷风控主要分为贷前、贷中、贷后三部分 1.贷前风险识别，贷中的风险监控与预警 python 数据结构的用法##python字符串的用法 ###一、字符串的拼接 字符串的格式化(%d, %f)print(‘%s %s’ % (‘hello’,’world’))hello world format()拼接方式简洁版s1=’hello {}! my name is {}.’.format(‘world’,’python猫’)print(s1)hello world! my name is python猫.对号入座s2=’hello {0}! my name is {1}.’.format(‘world’,’python猫’)print(s2)hello world my name is python猫 s3=’hello {name1}! my name is {name2}.’.format(name1=’world’,name2=’python猫’)print(s2)hello world! my name is python猫. join()拼接方式str_list=[‘hello’,’world’]str_join1=’’.join(str_list)print(str_join1)hello worldstr_join2=’-‘.join(str_list)print(str_join2)hello-world f-string方式name=’world’myname=’python_cat’words=f’hello {name}. my name is {myname}.’print(words)hello world. my name is python_cat 二、 将字符串拆分 split(a,b) a为拆分字符串, b为最大不能超过的拆分次数s=’hello world’s.split() &gt;&gt;&gt; [‘hello’, ‘world’]s.split(‘ ‘) &gt;&gt;&gt;[‘hello’,’world’]s.split(‘ ‘) &gt;&gt;&gt;[‘hello world’]s.split(‘world’)&gt;&gt;&gt;[‘hello’,] 三、字符串替换方法 python列表的用法 extend(L)： 将L中所有元素添加到对象列表中 insert(i,x):在指定位置插入一个元素 remove(x):删除列表中值为x的第一个元素 pop():从列表中删除指定位置的元素 sort():对列表中元素进行排序简单排序 x=[4,6,2,1,7,9]x.sort() y=sorted(x) reverse():逆向排序列表 count(x):返回x在列表中出现的次数 a=[1,2,3,4,5]del a[2] tuple([1,2,3]) –(1,2,3) 将列表转为元组 enumrate()函数:使用enumrate函数可以得到值与对应的索引位置for i，v in enumrate([‘tic’,’tac’,’toe’]) print i,v ###嵌套的列表推导式-mat=[[1,2,3],[4,5,6],[7,8,9],]-print [[row[i] for row in mat] for i in [0,1,2]]-zip(*mat)-[(1,4,7),(2,5,8),(3,6,9)] ##python的字典方法 clear()清除字典中的所有的项x.clear() copy()方法获得的是浅复制的副本，如果副本替换值的时候原始字典不会改变，如果副本不是替换而是修改的时候，原始字典也会改变。 deepcopy()方法 from copy import deepcopy使用deepcopy的方法就不用担心副本改变造成原始字典发生改变的情况了。 fromkeys()方法 使用给定的键建立新的字典{}.fromkeys([‘name’,’age’]){‘name’:None,’age’:None}dict.fromkeys([‘name’,’age’],’unknown’){‘name’:’unknown’,’age’:’unknown’} keys和iterkeyskeys方法将字典中的键以列表的形式返回 iterkeys返回针对键的迭代器 values和itervalues 《《《 》》》 dictvalues方法以列表的形式返回字典中的值 itervalues返回的是迭代器使用dict就可以将键-值对的字典形式还原成字典 items和iteritems方法items将字典中的键值对以列表的形式返回 iteritems 返回键值对对应的迭代器 pop方法用来获得对应于给定键的值，然后将这个键-值对从字典中移除d={‘title’: ‘python’,’url’:’http’,’spam’:0}d.pop(‘spam’)d {‘title’:’python’,’url’:’http’} update方法利用字典项更新另外一个字典：提供的字典中的项会被添加到旧的字典中，若有相同的键会进行覆盖d={‘a’:1,’b’:2,’c’:3}x={‘d’:4}d.update(x)d {‘a’:1,’b’:2,’c’:3,’d’:4} 序列解包这里序列解包或者可迭代解包：将多个值得序列解开，然后放到变量的序列中values=1,2,3x,y,z=valuesx 1 y 2 z 3 popitem方法 通过popitem方法可以获取到字典的键-值对，同时也删除了字典的键-值对。d={‘name’:’Robin’,’girlfriend’:’Mraion’}key,value=d.popitem() while 循环用来在任何条件为真的情况下重复执行一个代码块name=’’while not name: name=raw_input(‘please enter yourname:’)print ‘hello. %s!’ % name break (跳出循环)break语句用来终止循环语句，哪怕循环条件没有成为False也将停止循环语句 continue语句continue语句是跳出当前的循环块中的剩余语句，然后继续下一个循环。 pass语句pass语句其实什么也不做但是在语法上必须要站位的时候使用16.列表推导式—轻量级循环[x*x for x in range(10) if x%3==0][(x,y) for x in range(3) for y in range(3)]]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F19%2Flinux%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[linux简介以及常用命令linux的常用命令 文件目录操作命令 压缩/解压缩命令 常用系统命令 文件打包命令 搜索命令 关机命令 文件目录操作命令ls:显示文件和目录列表cd: 切换目录pwd: 显示当前工作目录mkdir: 创建工作目录touch: 生成新的空文件或者更改现有文件的时间cp: 复制文件或者目录mv: 移动文件或者目录、文件或者目录改名rm: 删除文件或者目录 cat和tac:滚动显示文本文件内容 cat: 用于从文件头到文件尾显示 tac: 用于从文件尾到文件头显示 more和less: 分屏显示文本文件内容 more :只能从文件头到文件尾显示 less: 可以使用PgUp和PgDn双向显示 head和tail:默认显示10行内容 head: 显示文本文件的前部的若干行 tail: 显示文本文件的后部若干行 grep: 用来从一个文件中找出匹配指定关键字的那一行，并送到标准输出。结合管道，我们通常用它来过滤搜索结果。 压缩、解压缩命令gzip和gunzip: 压缩和解压缩文件或目录 gzip: 压缩文件 gunzip: 解压缩文件zip和unzip:压缩和解压缩文件或目录 zip: 压缩文件 unzip: 解压缩文件 bzip2和bunzip2: 压缩和解压缩文件或目录 bzip2: 压缩文件 bunzip2: 解压缩文件 文件打包命令tar: 打包和解压 用于磁带机备份，也可以备份在一个硬盘文件上 主要参数: -c: 将文件备份过来 -v: 将过程输出 -x: 从一个文件中解出备份 范例 打包：tar -cvf myfile.tar somedirname 解包 ：tar -xvf myfile.tar 文件搜索命令which: 显示一个可执行文件的完整路径—which lswhere is :搜索一个可执行工具及其相关配置、帮助locate: 显示所有文件名以及其所在路径包含关键字段的文件与目录用法find: 从指定路径下递归向下搜索文件 linux的用户和用户组操作 crontab 每周三的时间点 crontab -e -u root/3 free -m &gt;&gt; /tmp/mem.log /3 date &gt;&gt; /tmp/mem.log]]></content>
  </entry>
  <entry>
    <title><![CDATA[python数据的可视化]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F08%2F%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[没有任何的办法，只有一点点的写下去，虽然心很累，虽然不知道前面的什么，但是多记录一点总会是自己的财富，坚持吧！ 向客户展示数据产品，经常用到的直接有效的统计工具就是可视化图，在python中数据可视化的库有挺多的，这里推荐几个比较常用的： Matplotlib Plotly Seaborn Ggplot Bokeh Pyechart Pygal这里我主要推荐两个，分别是 bokeh、pyecharts。 pyecharts来源http://pyecharts.org/#/zh-cn/charts_base?id=heatmap%ef%bc%88%e7%83%ad%e5%8a%9b%e5%9b%be%ef%bc%89 pyecharts是一个用于生成Echarts图表的类库 echarts是百度开源的一个数据可视化JS库，主要用于数据的可视化。pyecharts是一个用于生成Echarts图表的类库。实际上就是Echarts与python的对接。使用pyecharts可以生成独立的网页，也可以在flask,Django中集成使用。 pyecharts中基本的图表有多种类，这里重点介绍常用的图表： Bar(柱状图/条形图) Bar3D(3D柱状图) Boxplot(箱型图) Geo(地理坐标系) Graph(关系图) HeatMap(热力图) Kline/Candlestick(K线图) Pie(饼图) Radar(雷达图) Scatter(散点图) wordCloud(词云图) Bar(柱状图/条形图)柱状图Bar.add() add(name, x_axis, y_axis, is_stack=False, bar_category_gap=’20%’) 12345678from pyecharts import Barattr=[&quot;衬衫&quot;, &quot;羊毛衫&quot;, &quot;雪纺衫&quot;, &quot;裤子&quot;, &quot;高跟鞋&quot;, &quot;袜子&quot;]v1=[5,20,36,10,75,90]v2=[10,25,8,60,20,80]bar=Bar(&quot;柱状图示例&quot;)bar.add(&quot;商家A&quot;, attr, v1, is_stack=True)bar.add(&quot;商家B&quot;, attr, v2, is_stack=True)bar.render() 使用标记点和标记线 123456from pyecharts import Bar bar=Bar(&quot;标记点和标记线示例&quot;)bar.add(&quot;商家A&quot;, attr, v1, mark_point=[&quot;average&quot;])bar.add(&quot;商家B&quot;, attr, v2, mark_line=[&quot;min&quot;,&quot;max&quot;])bar.render() 条形图is_convert交换xy轴 12345from pyecharts import Barbar=Bar(&quot;x轴与y轴交换&quot;)bar.add(&quot;商家A&quot;, attr, v1)bar.add(&quot;商家B&quot;, attr, v2, is_convert=True)bar.render() Boxplot(箱型图) 箱型图是一种用作显示一组数据分散情况资料的统计图。它能显示出一组数据的最大值、最小值、中位数、下四分位数以及上四分位数。 Boxplot.add()方法 add(name, x_axis, y_axis,**kwargs) 可自行计算出所需要的五个值，也可通过内置prepare_data()转换,可将列表中的数据转换成嵌套的[min, Q1, median, Q3, max],如下所示： 123456789101112131415161718192021 from pyecharts import Boxplot boxplot=Boxplot(&quot;箱型图&quot;) x_axis=[&apos;expr1&apos;,&apos;expr2&apos;,&apos;expr3&apos;,&apos;expr4&apos;,&apos;expr5&apos;] y_axis=[ [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960], [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800], [880, 880, 880, 860, 720, 720, 620, 860, 970, 950, 880, 910, 850, 870, 840, 840, 850, 840, 840, 840], [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780], [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870]]_yaxis=boxplot.prepare_data(y_axis) #转换数据boxplot.add(&quot;boxplot&quot;, x_axis, _yaxis)boxplot.render() 或者可以直接在add()转换1234567891011121314151617181920 from pyecharts import Boxplotboxplot = Boxplot(&quot;箱形图&quot;)x_axis = [&apos;expr1&apos;, &apos;expr2&apos;]y_axis1 = [ [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960], [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800],]y_axis2 = [ [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780], [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870]]boxplot.add(&quot;cat1&quot;,x_axis, boxplot.prepare_data(y_axis1))boxplot.add(&quot;cat2&quot;,x_axis, boxplot.prepare_data(y_axis2))boxplot_render() Geo(地理坐标系) 地理坐标系组件用于地图的绘制，支持在地理坐标系上绘制散点图，线集 Geo.add()方法1add(name,attr, value, type=&quot;scatter&quot;,maptype=&apos;china&apos;,coordinate_region=&quot;中国&quot;,symbol_size=12,border_color=&apos;#323c48&apos;,geo_emphsis_color=&apos;#2a333d&apos;,geo_cities_coords=None,is_roam=True,**kwargs) 1234567891011121314151617181920212223242526272829303132333435363738394041424344 from pyecharts import Geo data=[ (&quot;海门&quot;, 9),(&quot;鄂尔多斯&quot;, 12),(&quot;招远&quot;, 12),(&quot;舟山&quot;, 12),(&quot;齐齐哈尔&quot;, 14),(&quot;盐城&quot;, 15), (&quot;赤峰&quot;, 16),(&quot;青岛&quot;, 18),(&quot;乳山&quot;, 18),(&quot;金昌&quot;, 19),(&quot;泉州&quot;, 21),(&quot;莱西&quot;, 21), (&quot;日照&quot;, 21),(&quot;胶南&quot;, 22),(&quot;南通&quot;, 23),(&quot;拉萨&quot;, 24),(&quot;云浮&quot;, 24),(&quot;梅州&quot;, 25), (&quot;文登&quot;, 25),(&quot;上海&quot;, 25),(&quot;攀枝花&quot;, 25),(&quot;威海&quot;, 25),(&quot;承德&quot;, 25),(&quot;厦门&quot;, 26), (&quot;汕尾&quot;, 26),(&quot;潮州&quot;, 26),(&quot;丹东&quot;, 27),(&quot;太仓&quot;, 27),(&quot;曲靖&quot;, 27),(&quot;烟台&quot;, 28), (&quot;福州&quot;, 29),(&quot;瓦房店&quot;, 30),(&quot;即墨&quot;, 30),(&quot;抚顺&quot;, 31),(&quot;玉溪&quot;, 31),(&quot;张家口&quot;, 31), (&quot;阳泉&quot;, 31),(&quot;莱州&quot;, 32),(&quot;湖州&quot;, 32),(&quot;汕头&quot;, 32),(&quot;昆山&quot;, 33),(&quot;宁波&quot;, 33), (&quot;湛江&quot;, 33),(&quot;揭阳&quot;, 34),(&quot;荣成&quot;, 34),(&quot;连云港&quot;, 35),(&quot;葫芦岛&quot;, 35),(&quot;常熟&quot;, 36), (&quot;东莞&quot;, 36),(&quot;河源&quot;, 36),(&quot;淮安&quot;, 36),(&quot;泰州&quot;, 36),(&quot;南宁&quot;, 37),(&quot;营口&quot;, 37), (&quot;惠州&quot;, 37),(&quot;江阴&quot;, 37),(&quot;蓬莱&quot;, 37),(&quot;韶关&quot;, 38),(&quot;嘉峪关&quot;, 38),(&quot;广州&quot;, 38), (&quot;延安&quot;, 38),(&quot;太原&quot;, 39),(&quot;清远&quot;, 39),(&quot;中山&quot;, 39),(&quot;昆明&quot;, 39),(&quot;寿光&quot;, 40), (&quot;盘锦&quot;, 40),(&quot;长治&quot;, 41),(&quot;深圳&quot;, 41),(&quot;珠海&quot;, 42),(&quot;宿迁&quot;, 43),(&quot;咸阳&quot;, 43), (&quot;铜川&quot;, 44),(&quot;平度&quot;, 44),(&quot;佛山&quot;, 44),(&quot;海口&quot;, 44),(&quot;江门&quot;, 45),(&quot;章丘&quot;, 45), (&quot;肇庆&quot;, 46),(&quot;大连&quot;, 47),(&quot;临汾&quot;, 47),(&quot;吴江&quot;, 47),(&quot;石嘴山&quot;, 49),(&quot;沈阳&quot;, 50), (&quot;苏州&quot;, 50),(&quot;茂名&quot;, 50),(&quot;嘉兴&quot;, 51),(&quot;长春&quot;, 51),(&quot;胶州&quot;, 52),(&quot;银川&quot;, 52), (&quot;张家港&quot;, 52),(&quot;三门峡&quot;, 53),(&quot;锦州&quot;, 54),(&quot;南昌&quot;, 54),(&quot;柳州&quot;, 54),(&quot;三亚&quot;, 54), (&quot;自贡&quot;, 56),(&quot;吉林&quot;, 56),(&quot;阳江&quot;, 57),(&quot;泸州&quot;, 57),(&quot;西宁&quot;, 57),(&quot;宜宾&quot;, 58), (&quot;呼和浩特&quot;, 58),(&quot;成都&quot;, 58),(&quot;大同&quot;, 58),(&quot;镇江&quot;, 59),(&quot;桂林&quot;, 59),(&quot;张家界&quot;, 59), (&quot;宜兴&quot;, 59),(&quot;北海&quot;, 60),(&quot;西安&quot;, 61),(&quot;金坛&quot;, 62),(&quot;东营&quot;, 62),(&quot;牡丹江&quot;, 63), (&quot;遵义&quot;, 63),(&quot;绍兴&quot;, 63),(&quot;扬州&quot;, 64),(&quot;常州&quot;, 64),(&quot;潍坊&quot;, 65),(&quot;重庆&quot;, 66), (&quot;台州&quot;, 67),(&quot;南京&quot;, 67),(&quot;滨州&quot;, 70),(&quot;贵阳&quot;, 71),(&quot;无锡&quot;, 71),(&quot;本溪&quot;, 71), (&quot;克拉玛依&quot;, 72),(&quot;渭南&quot;, 72),(&quot;马鞍山&quot;, 72),(&quot;宝鸡&quot;, 72),(&quot;焦作&quot;, 75),(&quot;句容&quot;, 75), (&quot;北京&quot;, 79),(&quot;徐州&quot;, 79),(&quot;衡水&quot;, 80),(&quot;包头&quot;, 80),(&quot;绵阳&quot;, 80),(&quot;乌鲁木齐&quot;, 84), (&quot;枣庄&quot;, 84),(&quot;杭州&quot;, 84),(&quot;淄博&quot;, 85),(&quot;鞍山&quot;, 86),(&quot;溧阳&quot;, 86),(&quot;库尔勒&quot;, 86), (&quot;安阳&quot;, 90),(&quot;开封&quot;, 90),(&quot;济南&quot;, 92),(&quot;德阳&quot;, 93),(&quot;温州&quot;, 95),(&quot;九江&quot;, 96), (&quot;邯郸&quot;, 98),(&quot;临安&quot;, 99),(&quot;兰州&quot;, 99),(&quot;沧州&quot;, 100),(&quot;临沂&quot;, 103),(&quot;南充&quot;, 104), (&quot;天津&quot;, 105),(&quot;富阳&quot;, 106),(&quot;泰安&quot;, 112),(&quot;诸暨&quot;, 112),(&quot;郑州&quot;, 113),(&quot;哈尔滨&quot;, 114), (&quot;聊城&quot;, 116),(&quot;芜湖&quot;, 117),(&quot;唐山&quot;, 119),(&quot;平顶山&quot;, 119),(&quot;邢台&quot;, 119),(&quot;德州&quot;, 120), (&quot;济宁&quot;, 120),(&quot;荆州&quot;, 127),(&quot;宜昌&quot;, 130),(&quot;义乌&quot;, 132),(&quot;丽水&quot;, 133),(&quot;洛阳&quot;, 134), (&quot;秦皇岛&quot;, 136),(&quot;株洲&quot;, 143),(&quot;石家庄&quot;, 147),(&quot;莱芜&quot;, 148),(&quot;常德&quot;, 152),(&quot;保定&quot;, 153), (&quot;湘潭&quot;, 154),(&quot;金华&quot;, 157),(&quot;岳阳&quot;, 169),(&quot;长沙&quot;, 175),(&quot;衢州&quot;, 177),(&quot;廊坊&quot;, 193), (&quot;菏泽&quot;, 194),(&quot;合肥&quot;, 229),(&quot;武汉&quot;, 273),(&quot;大庆&quot;, 279)]geo=Geo(&quot;全国城市空气质量&quot;，&quot;data from pm2.5&quot;, title_color=&quot;#fff&quot;,title_pos=&quot;center&quot;,width=1200,height=600,background_color=&apos;#404a59&apos;,)attr,value = gep.cast(data)geo.add(&quot;&quot;,attr,value,visual_range=[0,200],visual_text_color=&quot;#fff&quot;,symbol_size=15,is_visualmap=True,)geo.render()]]></content>
      <tags>
        <tag>数据的可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[群发QQ邮件]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F07%2Fqq-spider%2F</url>
    <content type="text"><![CDATA[多学习点实际有用的东西，让我们的生活变得更加的方便 一、抓取qq群里的成员信息本文主要介绍如何通过python实现自动群发邮件到qq群里面所有成员的邮箱中。 首先需要了解的是获得qq群里所有成员的qq号123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138from lxml import etreeimport timefrom selenium import webdriverclass qqGroupSpider(): &apos;&apos;&apos; Q群爬虫类 &apos;&apos;&apos; def __init__(self, driver, qq, passwd, qqgroup): &apos;&apos;&apos; 初始化根据用户信息登录到Q群管理界面 :param driver: :param qq: :param passwd: :param qqgroup: :param writefile: &apos;&apos;&apos; url = &quot;https://qun.qq.com/member.html#gid=&#123;&#125;&quot;.format(qqgroup) self.driver = driver driver.delete_all_cookies() driver.get(url) time.sleep(1) driver.switch_to.frame(&quot;login_frame&quot;) # 进入登录iframe time.sleep(1) change = driver.find_element_by_id(&quot;switcher_plogin&quot;) change.click() driver.find_element_by_id(&apos;u&apos;).clear() # 选择用户名框 driver.find_element_by_id(&apos;u&apos;).send_keys(qq) driver.find_element_by_id(&apos;p&apos;).clear() driver.find_element_by_id(&apos;p&apos;).send_keys(passwd) driver.find_element_by_class_name(&quot;login_button&quot;).click() time.sleep(1) def scroll_foot(self, driver): &apos;&apos;&apos; 控制屏幕向下滚动到底部 :param driver: :return: &apos;&apos;&apos; js = &quot;var q=document.documentElement.scrollTop=100000&quot; return driver.execute_script(js) def getTbodyList(self, driver): print(&quot;getTbodyList()函数运行过&quot;) return driver.find_elements_by_xpath(&apos;//div[@class=&quot;group-memeber&quot;]//tbody[contains(@class,&quot;list&quot;)]&apos;) def parseTbody(self, html): &apos;&apos;&apos; 解析tbody里面的内容，一个tbody里面有多个成员， 解析完成后，返回成员基本情况的列表 :param html: :return: &apos;&apos;&apos; # selector = etree.HTML(html) print(&quot;parseTbody()函数运行过&quot;) memberLists = [] for each in html: memberList = each.find_elements_by_xpath(&apos;tr[contains(@class,&quot;mb mb&quot;)]&apos;) memberLists += memberList print(&quot;memberLists长度为：&#123;&#125;&quot;.format(len(memberLists))) memberLists_data = [] for each in memberLists: memberLists_data.append(self.parseMember(each)) return memberLists_data def parseMember(self, mb): &apos;&apos;&apos; 解析每个人各项描述，以逗号隔开，返回一个成员的基本情况 :param mb: :return: &apos;&apos;&apos; print(&quot;parseMember()函数运行过&quot;) td = mb.find_elements_by_xpath(&apos;td&apos;) print(&quot;td长度为：&#123;&#125;&quot;.format(len(td))) qId = td[1].text.strip() nickName = td[2].find_element_by_xpath(&apos;span&apos;).text.strip() card = td[3].find_element_by_xpath(&apos;span&apos;).text.strip() qq = td[4].text.strip() sex = td[5].text.strip() qqAge = td[6].text.strip() joinTime = td[7].text.strip() lastTime = td[8].text.strip() a = (qId + &quot;|&quot; + qq + &quot;|&quot; + nickName + &quot;|&quot; + card + &quot;|&quot; + sex + &quot;|&quot; + qqAge + &quot;|&quot; + joinTime + &quot;|&quot; + lastTime) print(a) return a def parseAndWrite(self, tbody): &apos;&apos;&apos; 解析HTML中的tbody，解析完成后写入到本地文件 :param tbody: :return: &apos;&apos;&apos; print(&quot;parseAndWrite()函数运行过&quot;) memberList = self.parseTbody(tbody) with open(&quot;C:\\Users\\lenovo\\Desktop\\qq\\shangda.txt&quot;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f: for each in memberList: f.write(str(each) + &quot;\n&quot;)def main(): qq = str(input(&quot;请输入您的qq号:&quot;)) passwd = str(input(&quot;请输入您的qq密码:&quot;)) qqgroup = str(input(&quot;请输入您的qq群号:&quot;)) driver = webdriver.Chrome() spider = qqGroupSpider(driver, qq, passwd, qqgroup) time.sleep(10) # 找到QQ群的人数，这里由于群人数被隐藏获取不是很方便。可以目测获得 qqNum = 307 print(&quot;QQ群人数为：&quot; + str(qqNum)) curren_qq_num = 0 prelen = 0 while curren_qq_num != qqNum: curren_qq_num = len(driver.find_elements_by_xpath(&apos;//*[@id=&quot;groupMember&quot;]//td[contains(@class,&quot;td-no&quot;)]&apos;)) # 不停的向下滚动屏幕，直到底部 spider.scroll_foot(driver) # 每次滚动休息1秒 time.sleep(1) tlist = spider.getTbodyList(driver) spider.parseAndWrite(tlist[prelen:]) prelen = len(tlist) # 更新tbody列表的长度 driver.quit()if __name__ == &apos;__main__&apos;: main() 代码来源：https://blog.csdn.net/qq_38251616/article/details/82963395 二、python实现自动发送邮件其次介绍如何使用python自动发送邮件？注意：Python自动发送邮件需要注意的我们需要首先开启发送方邮箱的smtp相关服务，否则会出现以下报错： SMTPAuthenticationError: (535,’Error: authentication failed’) SMTPAuthenticationError: (550,’\xd3\xc3\xbb\xa7\xce\xde\xc8\xa8\xb5\xc7\xc2\xbd’) 以qq邮箱作为发送邮箱为例，qq邮箱的smtp服务开启 完成smtp服务的开启之后我们可以通过下面的代码实现简单的邮件发送12345678910111213141516171819202122232425262728293031 #其中的smtplib，email模块都是python自带的无需安装#coding=utf-8import smtplibfrom email.mime.text import MIMETextfrom email.header import Headerdef send_email(SMTP_host, from_account, from_password, to_account, subject, content): #注意：这里的passwd是开启smtp后给的验证码 # 1. 实例化SMTP smtp = smtplib.SMTP() # 2. 链接邮件服务器 smtp.connect(SMTP_host) # 3. 配置发送邮箱的用户名和密码 smtp.login(from_account, from_password) # 4. 配置发送内容msg msg = MIMEText(content, &apos;plain&apos;, &apos;utf-8&apos;) msg[&apos;Subject&apos;] = Header(subject,&apos;utf-8&apos;) msg[&apos;From&apos;] = from_account msg[&apos;To&apos;] = to_account # 5. 配置发送邮箱，接受邮箱，以及发送内容 smtp.sendmail(from_account, to_account, msg.as_string()) # 6. 关闭邮件服务 smtp.quit()if __name__ == &apos;__main__&apos;: send_email(&quot;smtp.163.com&quot;, &quot;from_account&quot;, &quot;from_pssword&quot;,&quot;to_account&quot;, &quot;I want to talk to u&quot;, &quot;In this semester&quot;) 代码来源：https://www.cnblogs.com/lesleysbw/p/5897224.html 三、完整的实现python向qq群成员发送邮箱代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206 import time from selenium import webdriver import smtplib from email.mime.text import MIMEText from email.header import Header class qqGroupSpider(): &apos;&apos;&apos; Q群爬虫类 &apos;&apos;&apos; def __init__(self, driver, qq, passwd, qqgroup): &apos;&apos;&apos; 初始化根据用户信息登录到Q群管理界面 :param driver: :param qq: :param passwd: :param qqgroup: :param writefile: &apos;&apos;&apos; url = &quot;https://qun.qq.com/member.html#gid=&#123;&#125;&quot;.format(qqgroup) self.driver = driver driver.delete_all_cookies() driver.get(url) time.sleep(1) driver.switch_to.frame(&quot;login_frame&quot;) # 进入登录iframe time.sleep(1) change = driver.find_element_by_id(&quot;switcher_plogin&quot;) change.click() driver.find_element_by_id(&apos;u&apos;).clear() # 选择用户名框 driver.find_element_by_id(&apos;u&apos;).send_keys(qq) driver.find_element_by_id(&apos;p&apos;).clear() driver.find_element_by_id(&apos;p&apos;).send_keys(passwd) driver.find_element_by_class_name(&quot;login_button&quot;).click() time.sleep(1) def scroll_foot(self, driver): &apos;&apos;&apos; 控制屏幕向下滚动到底部 :param driver: :return: &apos;&apos;&apos; js = &quot;var q=document.documentElement.scrollTop=100000&quot; return driver.execute_script(js) def getTbodyList(self, driver): print(&quot;getTbodyList()函数运行过&quot;) return driver.find_elements_by_xpath(&apos;//div[@class=&quot;group-memeber&quot;]//tbody[contains(@class,&quot;list&quot;)]&apos;) def parseTbody(self, html): &apos;&apos;&apos; 解析tbody里面的内容，一个tbody里面有多个成员， 解析完成后，返回成员基本情况的列表 :param html: :return: &apos;&apos;&apos; # selector = etree.HTML(html) print(&quot;parseTbody()函数运行过&quot;) memberLists = [] for each in html: memberList = each.find_elements_by_xpath(&apos;tr[contains(@class,&quot;mb mb&quot;)]&apos;) memberLists += memberList print(&quot;memberLists长度为：&#123;&#125;&quot;.format(len(memberLists))) memberLists_data = [] for each in memberLists: memberLists_data.append(self.parseMember(each)) return memberLists_data def parseMember(self, mb): &apos;&apos;&apos; 解析每个人各项描述，以逗号隔开，返回一个成员的基本情况 :param mb: :return: &apos;&apos;&apos; print(&quot;parseMember()函数运行过&quot;) td = mb.find_elements_by_xpath(&apos;td&apos;) print(&quot;td长度为：&#123;&#125;&quot;.format(len(td))) qId = td[1].text.strip() nickName = td[2].find_element_by_xpath(&apos;span&apos;).text.strip() card = td[3].find_element_by_xpath(&apos;span&apos;).text.strip() qq = td[4].text.strip() sex = td[5].text.strip() qqAge = td[6].text.strip() joinTime = td[7].text.strip() lastTime = td[8].text.strip() a = (qId + &quot;|&quot; + qq + &quot;|&quot; + nickName + &quot;|&quot; + card + &quot;|&quot; + sex + &quot;|&quot; + qqAge + &quot;|&quot; + joinTime + &quot;|&quot; + lastTime) print(a) return a def parseAndWrite(self, tbody): &apos;&apos;&apos; 解析HTML中的tbody，解析完成后写入到本地文件 :param tbody: :return: &apos;&apos;&apos; print(&quot;parseAndWrite()函数运行过&quot;) memberList = self.parseTbody(tbody) with open(&quot;C:\\Users\\lenovo\\Desktop\\qq\\shangda.txt&quot;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f: for each in memberList: f.write(str(each) + &quot;\n&quot;)def main(): qq = str(input(&quot;请输入您的qq号:&quot;)) passwd = str(input(&quot;请输入您的qq密码:&quot;)) qqgroup = str(input(&quot;请输入您的qq群号:&quot;)) driver = webdriver.Chrome() spider = qqGroupSpider(driver, qq, passwd, qqgroup) time.sleep(10) # 找到QQ群的人数，这里由于群人数被隐藏获取不是很方便。可以目测获得 qqNum = 307 print(&quot;QQ群人数为：&quot; + str(qqNum)) curren_qq_num = 0 prelen = 0 while curren_qq_num != qqNum: curren_qq_num = len(driver.find_elements_by_xpath(&apos;//*[@id=&quot;groupMember&quot;]//td[contains(@class,&quot;td-no&quot;)]&apos;)) # 不停的向下滚动屏幕，直到底部 spider.scroll_foot(driver) # 每次滚动休息1秒 time.sleep(1) tlist = spider.getTbodyList(driver) spider.parseAndWrite(tlist[prelen:]) prelen = len(tlist) # 更新tbody列表的长度 driver.quit() def send_email(SMTP_host, from_account, from_password, to_account, subject, content): #注意：这里的passwd是开启smtp后给的验证码 # 1. 实例化SMTP smtp = smtplib.SMTP() # 2. 链接邮件服务器 smtp.connect(SMTP_host) # 3. 配置发送邮箱的用户名和密码 smtp.login(from_account, from_password) # 4. 配置发送内容msg msg = MIMEText(content, &apos;plain&apos;, &apos;utf-8&apos;) msg[&apos;Subject&apos;] = Header(subject,&apos;utf-8&apos;) msg[&apos;From&apos;] = from_account msg[&apos;To&apos;] = to_account # 5. 配置发送邮箱，接受邮箱，以及发送内容 smtp.sendmail(from_account, to_account, msg.as_string()) # 6. 关闭邮件服务 smtp.quit()#单进程if __name__ == &apos;__main__&apos;: main() with open(&quot;&quot;,&apos;r&apos;) as f: for line in f.readlines(): m=line.split(&apos;|&apos;)[1] smtp_host = smtp.qq.com from_account=your_account from_pssword=your_password to_account=m@qq.com send_email(&quot;smtp_host&quot;, &quot;from_account&quot;, &quot;from_pssword&quot;,&quot;to_account&quot;, &quot;I want to talk to u&quot;, &quot;In this semester&quot;) #多进程 改写send_email函数 def send_email(to_account): #注意：这里的passwd是开启smtp后给的验证码 # 1. 实例化SMTP SMTP_host=&apos;smtp.qq.com&apos; from_account=&apos;1159520248@qq.com&apos; from_password=&apos;ljahyylibmzxbabd&apos; subject=&apos;I want to talk to u&apos; content=&apos;In this semester&apos; smtp = smtplib.SMTP() # 2. 链接邮件服务器 smtp.connect(SMTP_host) # 3. 配置发送邮箱的用户名和密码 smtp.login(from_account, from_password) # 4. 配置发送内容msg msg = MIMEText(content, &apos;plain&apos;, &apos;utf-8&apos;) msg[&apos;Subject&apos;] = Header(subject,&apos;utf-8&apos;) msg[&apos;From&apos;] = from_account msg[&apos;To&apos;] = to_account # 5. 配置发送邮箱，接受邮箱，以及发送内容 smtp.sendmail(from_account, to_account, msg.as_string()) from multiprocessing import Pool if __name__ == &apos;__main__&apos;: main() m=[] with open(&quot;&quot;,&apos;r&apos;) as f: for line in f.readlines(): m=line.split(&apos;|&apos;)[1]+&apos;@qq.com&apos; m.append(m) pool=Pool(4) pool.map(send_email, m)]]></content>
      <tags>
        <tag>pthon爬虫</tag>
        <tag>QQ群号</tag>
        <tag>自动发送邮件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫篇(二)]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F05%2Fpython_scrapy%2F</url>
    <content type="text"><![CDATA[Scrapy的框架爬虫提问： 为什么使用scrapy框架来写爬虫 ？ 在python爬虫中：requests + selenium 可以解决目前90%的爬虫需求，难道scrapy 是解决剩下的10%的吗？显然不是。scrapy框架是为了让我们的爬虫更强大、更高效。但是对于一些中小型的爬虫任务来讲，Scrapy确实是非常好的选择，它避免了我们来写一些重复的代码，并且有着出色的性能。我们自己写代码的时候，比如为了提高爬取效率，每次都自己码多线程或异步等代码，大大浪费了开发时间。这时候使用已经写好的框架是再好不过的选择了，我们只要简单的写写解析规则和pipeline等就好了。 scrapy是python开发的一个快速、高层次的屏幕抓取和web抓取的框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。scrapy吸引人的地方在于它是一个框架，任何人都可以根据需要方便的修改。它也提供了多种类型爬虫的基类，如BaseSpider、sitemap爬虫等，最新版本还提供web2.0爬虫的支持。 流程框架 抓取第一个页面请求第一页的URL并得到源代码，进行下一页分析 获取内容和下一页的链接分析源代码，提取翻页内容，获取下一页链接等待进一步爬取 翻页爬取请求下一页的信息，分析内容并请求下一页的链接 保存运行结果将爬取文件存为特定的格式的文件或者存入数据库 scrapy框架Scrapy是一个快速的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用来数据挖掘，舆情分析和自动化测试。 引擎（scrapy Engine）:用来处理整个系统的数据流处理，触发事务 调度器（Scheduler）:用来接受引擎发过来的请求，压入队列中，并在引擎再次请求时候返回。可以决定下载器下一步要下载的网址并去除重复网址 下载器（Downloader）:用来下载网页内容，并将网页内容返回给爬虫（spiders）。 爬虫（Spiders）：从特定网页中提取出需要的信息。可以用它来制定特定网页的解析规则，提取特定的实体（item）或者URL链接。每一个spider负责一个或者多个特定的网站 项目管道（item Pipeline）:负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。 下载器中间件(Downloader Middlewares):位于Scrapy引擎和下载器之间的子框架，主要是处理Scrapy引擎与下载器之间的请求以及响应。 爬虫中间件（Spider Middlewares）:介于Scrapy引擎和爬虫之间的框架，主要工作是处理爬虫的响应输入和请求输出。 调度中间件（Scheduler Middewares）:介于Scrapy引擎和调度之间的中间件，处理从scrapy引擎发送的调度请求和响应。 建立scrapy爬虫项目具体代码及命令 生成scrapy工程项目，项目名为Lspiderscrapy startproject Lspider 进入生成的工程目录中cd Lspider 生成爬虫文件，名为L,爬虫网站域名m.comscrapy genspider -t basic L m.com 运行爬虫程序 scrapy crawl 爬虫名当运行单个爬虫文件 Scrapy runspider first.py 文件说明： scrapy.cfg 项目的主配置信息，用来部署scrapy时使用，爬虫相关的配置信息在settings.py文件中。 items.py 设置数据存储模板，用于结构化数据，如：Django的Model pipelines 数据处理行为，如：一般结构化的数据持久化 settings.py 配置文件，如：递归的层数、并发数，延迟下载等。强调:配置文件的选项必须大写否则视为无效，正确写法USER_AGENT=‘xxxx‘ spiders 爬虫目录，如：创建文件，编写爬虫规则 注意：一般创建爬虫文件时，以网站域名命名 信运部职责和作用快速，正确，稳定的支撑公司业务的跨速发展 信运部主要系统结构 核心系统： CRM BOSS 现有系统的功能完善 开发新系统 软件+虚拟机规建部、业开部、维护部、服务部、计划部、厂商 软件+物理机]]></content>
      <tags>
        <tag>python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫篇(一)]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F11%2F05%2Fpython-spider-one%2F</url>
    <content type="text"><![CDATA[注：不要左顾右盼。慢慢积累，慢慢写吧。毕竟除了这样单调的努力，我什么也做不了。 爬虫原理简介 爬虫原理简介 爬虫主要分为三个过程 1. 针对对象发送请求 这里的对象主要分为静态网页和动态网页。静态网页通过get, post方法就可以搞定，如：豆瓣、糗事百科、腾讯新闻等动态网页主要会遇到Ajax异步加载的网页. 解决方法：1. requests.get方法解决，点击下方的加载更多，然后在network中找到它 的API请求. 2. selenium模拟浏览器法。个人认为在异步加载的动态网页中通过selenium方法会更加的方便，后面都会介绍具体的案例。 2. 获取响应内容 如果服务器能正常200响应, 则会得到一个Response 3. 解析内容 解析html内容： 正则表达式(re模块)，第三方解析库如 Beautifulsoup,Xpath, pyquery等解析json模块: 这种一般是通过抓包工具获得的数据包解析二进制数据： 以wb的方式写入文件 4. 保存数据文件形式：纯文本，json, xml等关系型数据库:Mysql, SqlServer非关系型数据库:Mongodb, Ridis 内容解析问题的方法Xpath, find_all, find以及正则表达式 XPath:是一门在XML文档中查找信息的语言。 Xpath可以用来在XML文档中对元素和属性进行遍历。在Xpath中，有七种类型的节点：元素、属性、文本、命名空间、处理指令、注释以及文档（根）节点。XML文档是被作为节点树来对待的。树的根被称为文档节点或者根节点。绝对路径：从根目录开始一级一级的查找，相对路径：从任意制定的位置开始查找应用代码: 1234567891011121314driver=webdriver.Chrome()driver.implicitly_wait(10)driver.get(url3)time.sleep(3)html=etree.HTML(driver.page_source)all_img_list=[]img_group_list = html.xpath(&quot;//img[contains(@id,&apos;J_normal&apos;)]&quot;)print(len(img_group_list))for img_group in img_group_list: img_of_group = img_group.xpath(&quot;.//@data-original | .//@data-img-back | .//@data-img-side&quot;) print(img_of_group) all_img_list.append(img_of_group[0])print(len(all_img_list))print(all_img_list) find_allfind_all(tag, attributes, recursive, text, limit, keywords) tage: 标签 返回一个列表 attrs: 属性参数 是用一个python字典封装一个标签的若干属性和对应的属性值，注意是对一个标签的属性，所以一定要指明这个标签 soup.find(attrs={‘data-custom’:’xxx’})以及 soup.find(attrs={‘class’:’xxx’})链接：https://www.jianshu.com/p/ef2f246cae46 注：find与find_all类似，主要区别在与find_all返回的是列表 正则表达式中常用的字符含义 来源: https://www.jb51.net/article/65286.htm （1）数量词的贪婪模式与非贪婪模式正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式”ab”如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词”ab?”，将找到”a”。注：我们一般使用非贪婪模式来提取。（2）反斜杠问题与大多数编程语言相同，正则表达式里使用”\”作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符”\”，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\”：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\”表示。同样，匹配一个数字的”\d”可以写成r”\d”。有了原生字符串，妈妈也不用担心是不是漏写了反斜杠，写出来的表达式也更直观勒。 re的核心函数compile()函数 函数定义：compile(pattern, flag=0) 函数描述：编译正则表达式pattern,然后返回正则表达式对象 pattern=re.compile() 预编译123456789re.match(pattern, string, flags=0)如果字符串的开头能匹配正则表达式，返回对应的match的对象，否则Nonere.search(pattern, string, flags=0)在字符串中查找，是否能匹配正则表达式，若是，返回对应的match对象，否则返回Nonere.split(pattern, string, maxsplit=0, flags=0)使用正则表达式分离字符串。如果括号将正则表达式括起来，那么匹配的字符串也会被列入到list中返回。Maxsplit是分离的次数，maxsplit=1表示分离一次，默认0,不限次数re. findall() 返回的是listre.sub( pattern ,repl ,string,count=0,): 替换 匹配对象的方法group方法 方法定义：group(num=0) 方法描述：返回这个匹配对象，或者特殊编号的字组123456789101112131415161718192021import res1 = &apos;我12345+abcde&apos;pattern字符串前加 “ r ” 表示原生字符串pattern = r&apos;\w+&apos;pattern_compile = re.compile(pattern)返回匹配的字符串result1 = re.match(pattern_compile, s1).group()返回匹配开始的位置result2 = re.match(pattern_compile, s1).start() 返回匹配结束的位置result3 = re.match(pattern_compile, s1).end() 返回一个元组包含匹配 (开始,结束) 的位置result4 = re.match(pattern_compile, s1).span() print(result1)print(result2)print(result3)print(result4)我1234506(0, 6) groups方法 方法定义：groups(default=None) 方法描述： 返回一个含有所有匹配子组的元组，匹配失败则返回空元组 123456789import res1 = &apos;我12345+abcde&apos;pattern字符串前加 “ r ” 表示原生字符串pattern = r&apos;(\w+)\+(\w+)&apos;pattern_compile = re.compile(pattern)返回含有所有子组的元组result1 = re.search(pattern_compile, s1).groups()print(result1)(&apos;我12345&apos;, &apos;abcde&apos;) 最后附加一个静态网页的数据抓取的案例 12345678910111213141516171819202122232425from bs4 import BeautifulSoupimport requestsimport timeurl = &apos;https://knewone.com/discover?page=&apos;def get_page(url, data=None): res = requests.get(url) soup = BeautifulSoup(res.text, &apos;lxml&apos;) images = soup.select(&apos;a.cover-inner &gt; img&apos;) titles = soup.select(&apos;section.content &gt; h4 &gt; a&apos;) links = soup.select(&apos;section.content &gt; h4 &gt; a&apos;) if data==None: for image, title, link in zip(images, titles, links): #zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。 data = &#123;&apos;image&apos;: image.get(&apos;src&apos;), &apos;title&apos;: title.get(&apos;title&apos;), &apos;link&apos;: link.get(&apos;href&apos;) &#125; print(data)def get_more_page(start, end): for one in range(start, end): get_page(url + str(one)) time.sleep(2)get_more_page(5, 15) Ajax异步加载动态页面问题 获取数据源问题 – 模拟登录, 异步加载 数据解析问题 正如上篇所提到的处理该问题主要有两种方法，在本节我们重点介绍第二种方法，因为我认为通过selenium模拟浏览器可以方便快速的解决该问题。 Selenium首先我们来介绍下selenium来源: https://blog.csdn.net/u010986776/article/details/79266448) 通常反爬的手段的方向，都是识别非浏览器客户端，而selenium所做的事情，恰恰是驱动真正的浏览器去执行请求和操作，只不过信号不是来源于鼠标，而是来源于selenium的API（selenium本是一个自动化的测试工具） 自然人用户能做的一切，selenium几乎都驱动浏览器去做，无论是否界面，包括输入、点击、滑动，等等。 然而到底是鼠标操作的浏览器发起请求还是API，对于服务端来说，是没有任何差别的 早期的时候流行的组合是selenium+phantomjs而不是selenium+chrome浏览器驱动，因为phantomjs是一款没有界面的浏览器，业界称作无头浏览器（headless），由于没有界面和渲染，其运行要大大优于有界面的浏览器，后来chrome和firefox也推出了无头模式，且其运行速度很流畅，phantomjs就告终了。 python利用selenium模拟浏览器抓取异步加载的页面信息获取数据源 模拟启动火狐/谷歌浏览器from selenium import webdriver调用键盘的按键操作需要引入keys包from selenium.webdriver.common.keys import Keys导入chrome项from selenium.webdiver.chrome.options import Options模拟鼠标动作from selenuim.webdriver import ActionChains 启动浏览器，有头模式 Drive=webdiver.Chrome()Driver.get(url)print Driver.page_sourceDriver.close() 关闭浏览器 创建chrome浏览器驱动，无头模式 from selenium.webdriver.chrome.options import Optionschrome_options=Options()chrome_options.add_argument(“–headless”)driver=webdriver.Chrome(chrome_options=chrome_options)driver.implicity_wait(10)driver.get(url) 其实在使用selenium模块处理异步加载问题主要使用以下代码,自动的将滚动条拉下来12345678模拟谷歌浏览器滚动条滚动for i in range(1, 6): js=&quot;var q=document.body.scrollTop=100000&quot; driver.execute_script(js) print(&apos;=====================================&apos;) time.sleep(3) driver.execute_script(&quot;window.scrollTo(0, document.body.scrollHeight);&quot;) time.sleep(2) 除此之外使用selenium模块解决的更多的还有反爬虫中的验证码登录问题 数据源的解析与获取选择器（来源：https://www.cnblogs.com/yxi-liu/p/selenium.html） find_element_by_id 按照id 查找 find_element_by_link_text 按照里面的文本查找 find_element_by_partial_link_text 按照文本的部分模糊查找 find_element_by_tag_name 按照标签名 find_element_by_class_name 按照类名 find_element_by_name 按照name的属性查找 find_element_by_css_selector css选择器的方式查找 find_element_by_xpath 按照路径茶渣 1234567891011from lxml import etreehtml=etrr.HTML(driver.page_source)all_img_list=[]img_group_list = html.xpath(&quot;//img[contains(@id,&apos;J_normal&apos;)]&quot;)print(len(img_group_list))for img_group in img_group_list: img_of_group = img_group.xpath(&quot;.//@data-original | .//@data-img-back | .//@data-img-side&quot;) print(img_of_group) all_img_list.append(img_of_group[0]) print(len(all_img_list)) print(all_img_list) 具体案例可见后文中的“QQ邮件的自动发送” 数据存储问题正如上文所说的将爬取下来的数据存储主要有三种方式： 存储到txt文件中 存储到csv,excel文件中 存储到mysql,mongodb数据库中 数据存储到txt文件中将数据存储到txt文件中基本上都是逐条存储进去方法1 1234def save_to_txt(item):with open(&apos;C:\\Users\\lenovo\\Desktop\\1\\刘睿\\狄仁杰.txt&apos;, &apos;a&apos;, encoding=&apos;utf-8&apos;) as f: f.write(item[&apos;date&apos;] + &apos;,&apos; + item[&apos;nickname&apos;] + &apos;,&apos; + item[&apos;city&apos;] + &apos;,&apos; + str(item[&apos;rate&apos;]) + &apos;,&apos; + item[&apos;comment&apos;] + &apos;\n&apos;) print(&apos;ok&apos;) 方法2使用json.dumps方法是将字典转化成str然后写入txt中 1234567891011121314151617def parse_one_page(html): pattern = re.compile(&quot;&lt;dd&gt;.*? board_index.*?&gt;(.*?)&lt;/i&gt;.*?data-src=&quot;(.*?)&quot;.*?name.*?a.*?&gt;(.*?)&lt;/a&gt;.*?star.*?(.*?)&lt;/p&gt;.*?releasetime.*?&gt;(.*?)&lt;/p&gt;.*?integer.*?&gt;(.*?)&lt;/i&gt;.*?fraction.*?&gt;(.*?)&lt;/i&gt;.*?&lt;/dd&gt;&quot;,re.S) items =re.findall(pattern,html) #生成一个生成器通过for in从生成器中调取数据 for item in items: yield&#123; &apos;&apos;&apos;yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器 当你调用这个函数的时候，函数内部的代码并不立马执行 ，这个函数只是返回一个生成器对象&apos;&apos;&apos;当你使用for进行迭代的时候，函数中的代码才会执行 &quot;index&quot;:item[0], &quot;image&quot;:item[1], &quot;title&quot;:item[2], &quot;actor&quot;:item[3].strip()[3:], &quot;time&quot;:item[4].strip()[5:], &quot;score&quot;:item[5]+item[6] &#125;def write_to_file(content): with open(&quot;result.text&quot;,&quot;a&quot;,encoding=&quot;utf-8&quot;) as f:#a相当于append表示将文件追加写入文末 f.write(json.dumps(content,ensure_ascii=False) + &quot;\n&quot;)#dumps是将dict转化成str格式，loads是将str转化成dict格式。 方法3 1234567891011121314151617181920def get_btc_dt(pages): f0 = open(&apos;title.txt&apos;, &apos;w&apos;) f1 = open(&apos;time.txt&apos;, &apos;w&apos;) for page in range(1, pages+1): url = &apos;http://www.8btc.com/bitcoin/page/&#123;&#125;&apos;.format(str(page)) web_data = requests.get(url, headers=headers) time.sleep(2) soup = BeautifulSoup(web_data.text, &apos;lxml&apos;) titles = soup.select(&apos;div.article-content.clearfix &gt; div.article-title.visible-md.visible-lg &gt; a&apos;) times = soup.select(&apos;div.article-content.clearfix &gt; div.article-info.clearfix &gt; span.pull-left.visible-sm.visible-xs&apos;) for time, title in zip(times, titles): time = time.text title = title.text print(time) f0.write(time) f0.write(&apos;\n&apos;) f1.write(title) f1.write(&apos;\n&apos;) f0.close() f1.close() 存储到csv,excel文件中主要是将由字典结构&amp;其内值为列表的数据通过pandas的DataFrame存入csv和excel中 方法1 123456789101112131415161718datalength = len(data)Date_d = np.zeros(datalength)Open = np.zeros(datalength)High = np.zeros(datalength)Low = np.zeros(datalength)Close = np.zeros(datalength)Volume = np.zeros(datalength)for i in range(len(data)): Date_d[i] = data[i][0] Open[i] = data[i][1] High[i] = data[i][2] Low[i] = data[i][3] Close[i] = data[i][4] Volume[i] = data[i][5]Date_d = Date_d / 1000000000df = pd.DataFrame(&#123;&apos;Close&apos;: Close, &apos;Date_d&apos;: Date_d,&apos;Open&apos;:Open,&apos;High&apos;:High,&apos;Low&apos;:Low,&apos;Volume&apos;:Volume&#125;)df.to_csv(&apos;C:\\Users\\lenovo\Desktop\\btc_yunbi.csv&apos;) 方法2：将由列表组成的,其内是字典的数据,可以将数据一条一条插入，也可以一起插入1234567891011121314151617result_list = []for item in item_list: result_dict = &#123;&#125; result_dict[&apos;title&apos;] = item[&apos;title&apos;].replace(&apos;&lt;span class=H&gt;&apos;, &apos;&apos;).replace(&apos;&lt;/span&gt;&apos;, &apos;&apos;) result_dict[&apos;url&apos;] = &apos;http:&apos; + item[&apos;detail_url&apos;] result_dict[&apos;location&apos;] = item[&apos;item_loc&apos;] result_dict[&apos;shop_name&apos;] = item[&apos;nick&apos;] result_dict[&apos;原价&apos;] = item[&apos;reserve_price&apos;] result_dict[&apos;现价&apos;] = item[&apos;view_price&apos;] print(result_dict) result_list.append(result_dict)return result_listdef write_data(self, result_list):with open(&apos;result.csv&apos;, &apos;w&apos;, encoding=&apos;UTF-8&apos;) as f: writer = csv.DictWriter(f, fieldnames=[&apos;title&apos;, &apos;原价&apos;, &apos;现价&apos;,&apos;shop_name&apos;, &apos;location&apos;, &apos;url&apos;]) writer.writeheader() writer.writerows(result_list) 存储到mysql, mongodb数据库中先在mysql数据库中建立好表格(表头以及类型)然后将数据逐条插入数据库中12345678910111213conn = pymysql.connect(host=&quot;127.0.0.1&quot;, user=&quot;root&quot;, passwd=&quot;341124&quot;, db=&quot;jd&quot;, charset=&quot;utf8&quot;)使用cursor()的方法获取操作游标cur = conn.cursor()for i in range(0, len(item[&apos;title&apos;])): try: cur.execute(&quot;INSERT INTO jingjing(title,href) VALUES (%s, %s)&quot;, [item[&apos;title&apos;][i], &quot;http:&quot;+item[&apos;href&apos;][i]]) conn.commit() print(&apos;ok&apos;) except Exception as e: #错误回滚 conn.rollback() conn.close() print(&apos;ok&apos;)]]></content>
      <tags>
        <tag>python爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一篇博客]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F10%2F30%2F%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[链接与图片简书每日一更 生活不仅眼前的苟且还有诗和远方 我们还要有更好的生活 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。12345678import pandas as pddata = pd.read(&apos;get_stock_basics.csv&apos;,encoding = &apos;utf8&apos;)print(data.head()) ts_code symbol name list_status list_date is_hs000001.SZ 1 平安银行 L 19910403 S000002.SZ 2 万科A L 19910129 S000004.SZ 4 国农科技 L 19910114 N000005.SZ 5 世纪星源 L 19901210 N]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2FBlack-House.github.io%2F2018%2F10%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
